Thank you for putting together such a thoughtful and socially impactful proposal — building an AI-powered sign language translator directly supports SDG 4 and SDG 10 by promoting inclusive communication and educational access for the deaf and hard-of-hearing community. Your technical approach, combining CNNs for spatial feature extraction with Transformer-based models for gesture sequence recognition, reflects a strong grasp of the challenges in real-time sign language interpretation. The use of Amharic sign language also makes the project uniquely relevant in the Ethiopian context, addressing a major gap in accessibility technologies.

To make the project more novel and practically useful, consider expanding the system to support bi-directional translation — translating not only from sign language to text/speech but also from text to animated sign avatars, which can benefit deaf users in reading-limited contexts. You could also enhance the model’s robustness by incorporating pose estimation techniques (e.g., MediaPipe, OpenPose) to track hand, body, and facial landmarks more precisely, especially in low-light or cluttered environments. If Amharic sign language datasets are limited, you might explore data synthesis using GANs or transfer learning from larger ASL datasets, gradually adapting to local variations. Lastly, partnering with local deaf communities and educators in the development and validation phases can ensure the system is culturally accurate and widely adoptable.
